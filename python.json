[
  {"id":1, "type":"单选题", "q":"下列选项中，不属于Python开发网络爬虫优势的是（）。", "opts":{"A":"语法简洁，容易上手","B":"开发效率高","C":"丰富的模块","D":"运行速度快、性能强"}, "a":"D", "desc":"Python 的运行速度相比 C/C++ 较慢，但开发效率极高。"},
  {"id":2, "type":"单选题", "q":"下列选项中，被称为主题网络爬虫的是（）。", "opts":{"A":"增量式网络爬虫","B":"通用网络爬虫","C":"深层爬虫","D":"聚焦网络爬虫"}, "a":"D", "desc":"聚焦网络爬虫（Focused Crawler）是指选择性地爬取与预先定义的主题相关的页面的网络爬虫。"},
  {"id":3, "type":"单选题", "q":"下列选项中，不属于防爬虫策略的是（）。", "opts":{"A":"添加User-agent字段","B":"降低访问频率","C":"反复使用同一IP抓取数据","D":"识别验证码"}, "a":"C", "desc":"反复使用同一IP抓取数据是爬虫的行为，容易被封禁，不属于“防爬虫策略”；反而是网站用来识别爬虫的特征。"},
  {"id":4, "type":"单选题", "q":"'发布人：张三 发布时间：2022-11-18 来源：图情信息中心'.split(' ')的执行结果是（）。", "opts":{"A":"['发布人：张三','','发布时间：2022-11-18','','来源：图情信息中心']","B":"['发布人：张三', '发布时间：2022-11-18', '来源：图情信息中心']","C":"('发布人：张三','','发布时间：2022-11-18', '', '来源：图情信息中心')","D":"('发布人：张三','发布时间：2022-11-18','来源：图情信息中心')"}, "a":"B", "desc":"split 方法返回的是列表，且默认以空格分割时不包含空字符串（如果字符串紧凑）。"},
  {"id":5, "type":"单选题", "q":"关于浏览器加载网页过程的说法，下列描述错误的是（）。", "opts":{"A":"浏览器通过DNS服务器查找被访问服务器对应的IP地址","B":"浏览器向DNS服务器解析的IP地址发送HTTP请求","C":"Web服务器将响应的HTML页面返回给DNS服务器","D":"浏览器会对HTML页面进行渲染并呈现给用户"}, "a":"C", "desc":"Web服务器将响应的HTML页面直接返回给“浏览器”，而不是DNS服务器。"},
  {"id":6, "type":"单选题", "q":"下列选项中，关于网络爬虫实现技术的描述错误的是（）。", "opts":{"A":"只有Python语言能够实现爬虫程序","B":"使用Python开发网络爬虫程序效率相对其他语言更高","C":"使用C++语言开发网络爬虫程序代码成型速度慢","D":"Java提供了众多解析网页的技术，对网页解析有着良好的支持"}, "a":"A", "desc":"Java、C++、Go 等多种语言都可以实现爬虫，Python 只是因其生态丰富而最受欢迎。"},
  {"id":7, "type":"单选题", "q":"下列选项中，用于解析域名的协议是（）。", "opts":{"A":"HTTP","B":"DNS","C":"FTP","D":"SMTP"}, "a":"B", "desc":"DNS (Domain Name System) 负责域名解析。"},
  {"id":8, "type":"单选题", "q":"下列选项中，表示超文本传输协议的是（）。", "opts":{"A":"File","B":"HTTP","C":"FTP","D":"Mailto"}, "a":"B", "desc":""},
  {"id":9, "type":"单选题", "q":"下列响应头中，用于告诉客户端资源文件的类型和编码的是（）。", "opts":{"A":"Connection","B":"Content-Encoding","C":"Content-Type","D":"Server"}, "a":"C", "desc":""},
  {"id":10, "type":"单选题", "q":"下列状态码中，表示服务器拒绝访问的是（）。", "opts":{"A":"402","B":"403","C":"404","D":"405"}, "a":"B", "desc":"403 Forbidden 表示服务器理解请求但拒绝执行。"},
  {"id":11, "type":"单选题", "q":"下列选项中，关于聚焦网络爬虫的描述错误的是（）。", "opts":{"A":"聚焦网络爬虫会随机抓取网页与主题相关的数据","B":"聚焦网络爬虫比通用网络爬虫目的性更强","C":"聚焦网络爬虫会根据一定的网页分析算法对网页进行筛选","D":"聚焦网络爬虫会根据预先设定的主题顺着某个垂直领域进行抓取"}, "a":"A", "desc":"聚焦爬虫是有目的、有策略的抓取，而不是“随机”抓取。"},
  {"id":12, "type":"单选题", "q":"下列选项中，在JSONPath中表示选取根对象的是（）。", "opts":{"A":"$","B":"/","C":"@","D":"*"}, "a":"A", "desc":""},
  {"id":13, "type":"单选题", "q":"下列选项中，关于HTTP协议的描述说法错误的是（）。", "opts":{"A":"HTTP协议能够高效准确的传送超文本资源","B":"若协议类型为HTTP，则每次连接可以处理多个请求","C":"HTTP协议中的每个请求都是独立的","D":"HTTP协议用于将Web服务器的超文本资源传送到浏览器中"}, "a":"B", "desc":"HTTP 1.0 默认为短连接（一次连接处理一个请求），HTTP 1.1 支持长连接，但本质上HTTP协议被设计为无状态、请求独立。"},
  {"id":14, "type":"单选题", "q":"以下哪个选项是文件传输协议，访问共享主机的文件资源（）。", "opts":{"A":"File","B":"FTP","C":"HTTP","D":"Mailto"}, "a":"B", "desc":""},
  {"id":15, "type":"单选题", "q":"下列选项中，关于动态页面的描述说法错误的是（）。", "opts":{"A":"动态网页的内容不一定呈现在网页源代码中","B":"动态网页的访问速度相较于静态网页更快","C":"采用动态网页技术的网站可以实现更多的功能","D":"动态网页相比静态网页，动态网页有数据库支撑"}, "a":"B", "desc":"动态网页需要服务器动态生成或客户端渲染，通常比静态网页慢。"},
  {"id":16, "type":"单选题", "q":"下列选项中，表示内容类型的字段是（）。", "opts":{"A":"Cache-Control","B":"Connection","C":"Content-Encoding","D":"Content-Type"}, "a":"D", "desc":""},
  {"id":17, "type":"单选题", "q":"下列选项中，用于标识客户端身份的是（）。", "opts":{"A":"HOST","B":"User-Agent","C":"Accept","D":"Referer"}, "a":"B", "desc":"User-Agent 包含了浏览器和操作系统信息，用于标识身份。"},
  {"id":18, "type":"单选题", "q":"关于响应状态码的描述说法错误的是（）。", "opts":{"A":"响应状态码代表服务器的响应状态","B":"响应状态码的作用是告知客户端请求Web资源的结果","C":"若服务器发生错误，用户便无法获取响应状态码","D":"当响应状态码为200时表示服务器接收请求并成功处理"}, "a":"C", "desc":"服务器发生错误时通常会返回 5xx 系列的状态码。"},
  {"id":19, "type":"单选题", "q":"下列选项中，关于Requests库post()函数的说法错误的是（）。", "opts":{"A":"如果请求数据类型为Json可通过参数json传递","B":"post()函数会根据传入的URL构建一个请求并将该请求发送给服务器","C":"post()函数通过参数data携带请求数据","D":"post()函数既可以发送GET请求也可以发送POST请求"}, "a":"D", "desc":"post() 函数专门用于发送 POST 请求。"},
  {"id":20, "type":"单选题", "q":"关于抓取静态网页实现技术的说法，下列描述错误的是（）。", "opts":{"A":"如果要抓取静态网页的数据，只需要获得网页的源代码即可","B":"通过urllib、urllib3和Requests等库抓取静态网页数据","C":"Requests库只能发送网络请求不能获取网页源码","D":"抓取静态网页数据的整个过程是模仿用户通过浏览器访问网页的过程"}, "a":"C", "desc":"Requests 库的 response.text 属性就是网页源码。"},
  {"id":21, "type":"单选题", "q":"下列选项中，不属于HTML元素组成的是（）。", "opts":{"A":"开始标签","B":"内容","C":"样式","D":"结束标签"}, "a":"C", "desc":"样式（CSS）通常独立于HTML元素结构，虽然可以通过style属性添加。HTML元素由标签和内容组成。"},
  {"id":22, "type":"单选题", "q":"下列选项中，表示图像标签的是（）。", "opts":{"A":"<html>","B":"<h1>","C":"<p>","D":"<img>"}, "a":"D", "desc":""},
  {"id":23, "type":"单选题", "q":"下列选项中，不属于请求行组成的是（）。", "opts":{"A":"请求方法","B":"URL","C":"协议版本","D":"请求数据"}, "a":"D", "desc":"请求行包含：方法、URL、协议版本。请求数据位于请求体中。"},
  {"id":24, "type":"单选题", "q":"下列选项中，关于静态页面的描述说法错误的是（）。", "opts":{"A":"静态网页的交互性较差，在功能方面有较大的限制","B":"静态网页的访问速度快，访问过程中无需连接数据库","C":"静态网页没有数据库的支持，内容更新与维护比较复杂","D":"静态网页的内容可根据用户信息进行定制化展示"}, "a":"D", "desc":"静态网页内容固定，无法根据用户信息动态定制。"},
  {"id":25, "type":"单选题", "q":"下列选项中，关于处理响应的描述说法错误的是（）。", "opts":{"A":"当服务器返回的响应状态码为200时，表明可以接收到由服务器返回的响应信息","B":"Response类的对象中封装了服务器返回的响应信息","C":"响应内容中只能包含文本内容","D":"若想获取响应的最终URL，可通过url属性获取"}, "a":"C", "desc":"响应内容也可以是二进制数据（如图片、视频），通过 .content 获取。"},
  {"id":26, "type":"单选题", "q":"下列选项中，用于在GET请求中传递查询字符串的是（）。", "opts":{"A":"params","B":"headers","C":"verify","D":"timeout"}, "a":"A", "desc":""},
  {"id":27, "type":"单选题", "q":"下列选项中，用于查看响应状态码的属性是（）。", "opts":{"A":"content","B":"headers","C":"text","D":"status_code"}, "a":"D", "desc":""},
  {"id":28, "type":"单选题", "q":"下列选项中，关于检测代理IP有效性的描述说法错误的是（）。", "opts":{"A":"当使用代理访问网站时，返回的状态码为200时表示代理可用","B":"当代理无效时，不能返回响应信息","C":"使用的代理IP通过参数proxies传递","D":"post()函数无法使用代理ip"}, "a":"D", "desc":"post() 函数同样支持 proxies 参数。"},
  {"id":29, "type":"单选题", "q":"（）会将数据包原封不动地转发给服务器，让服务器认为当前访问的用户只是一个普通客户端，而不是代理服务器。", "opts":{"A":"高度匿名代理服务器","B":"普通匿名代理服务器","C":"透明代理服务器","D":"所有选项均不对"}, "a":"A", "desc":"高度匿名代理（High Anonymity Proxy）不改变客户机的请求字段，服务器不知道使用了代理。"},
  {"id":30, "type":"单选题", "q":"关于定制请求头的描述说法错误的是（）。", "opts":{"A":"参数headers可以接收列表类型的数据","B":"定制的请求头需要由参数headers中传递","C":"get()函数和post()函数均可以添加定制请求头","D":"定制请求的目的是将发送的请求伪装成浏览器发送的请求"}, "a":"A", "desc":"headers 参数必须是字典（dict）类型。"},
  {"id":31, "type":"单选题", "q":"阅读代码：response.encoding = 'ISO-8859-1'，print(response.text)。上述程序运行后，会使用哪种编码方式返回文本（）。", "opts":{"A":"utf-8","B":"gbk","C":"gbk2312","D":"ISO-8859-1"}, "a":"D", "desc":""},
  {"id":32, "type":"单选题", "q":"requests库中，get()函数能用于设置是否启用SSL证书的参数是（）。", "opts":{"A":"url","B":"headers","C":"verify","D":"proxies"}, "a":"C", "desc":""},
  {"id":33, "type":"单选题", "q":"关于Requests库中get()函数的说法错误的是（）。", "opts":{"A":"get()函数既可以发送GET请求也可以发送POST请求","B":"get()函数中参数url是必选参数，该参数含义为请求地址","C":"get()函数会根据传入的URL构建一个请求","D":"使用get()函数发送GET请求时可以携带请求参数"}, "a":"A", "desc":"get() 只能发送 GET 请求。"},
  {"id":34, "type":"单选题", "q":"下列选项中，关于Cookie的描述错误的是（）。", "opts":{"A":"Cookie是一段文本数据，由一个名称和一个值组成","B":"Cookie的生存期可以由开发人员设置","C":"Cookie数据存储在网站服务器中","D":"Cookie是为了网站辨别用户身份、进行会话跟踪而存储的数据"}, "a":"C", "desc":"Cookie 存储在客户端（浏览器）中。"},
  {"id":35, "type":"单选题", "q":"下列正则表达式中，表示只能匹配任意数字的是（）。", "opts":{"A":"\\w","B":"\\s","C":"\\d","D":"\\b"}, "a":"C", "desc":"\\d 匹配数字 [0-9]。"},
  {"id":36, "type":"单选题", "q":"下列选项中，表示匹配前导字符0次或1次的是（）。", "opts":{"A":"?","B":"*","C":"+","D":"{n}"}, "a":"A", "desc":""},
  {"id":37, "type":"单选题", "q":"re模块中，对正则表达式进行预编译，从而生成一个代表正则表达式的Pattern对象（）。", "opts":{"A":"re.pattern()","B":"re.split()","C":"re.run()","D":"re.compile()"}, "a":"D", "desc":""},
  {"id":38, "type":"单选题", "q":"关于正则表达式的描述，说法错误的是（）。", "opts":{"A":"一条正则表达式也称为一个模式","B":"正则表达式匹配HTML时会根据其层次结构进行匹配","C":"正则表达式由普通字符、元字符或预定义字符集组成","D":"正则表达式是对字符串操作的一种逻辑公式"}, "a":"B", "desc":"正则表达式将 HTML 视为纯文本，不会识别 DOM 层次结构。"},
  {"id":39, "type":"单选题", "q":"下列选项中，关于设置代理服务器目的的说法正确的是（）。", "opts":{"A":"加快网络爬虫抓取数据的速度","B":"识别网站验证码","C":"降低访问网站速度","D":"防止IP被封禁"}, "a":"D", "desc":""},
  {"id":40, "type":"单选题", "q":"XPath路径表达式中，在搜索节点是会忽略层级关系的是（）。", "opts":{"A":"/","B":"//","C":"[]","D":"@"}, "a":"B", "desc":"// 表示从匹配选择的当前节点选择文档中的节点，而不考虑它们的位置。"},
  {"id":41, "type":"单选题", "q":"关于XPath的描述，说法错误的是（）。", "opts":{"A":"XPath基于XML或HTML的节点树定位目标节点所在的位置","B":"XPath是一种用于确定XML文档中部分节点位置的语言","C":"XPath匹配节点的方式与正则表达式匹配字符串的方式类似","D":"XPath通过路径表达式可以快速地定位与选取XML或HTML文档中的一个节点或者一组节点集"}, "a":"C", "desc":"XPath 基于树结构，正则基于字符流，原理完全不同。"},
  {"id":42, "type":"单选题", "q":"img标签中的什么属性，用于指图片地址（）。", "opts":{"A":"src","B":"href","C":"title","D":"alt"}, "a":"A", "desc":""},
  {"id":43, "type":"单选题", "q":"当正则表达式中包含能接受重复的限定符时，匹配尽可能少的字符，这被称为（）。", "opts":{"A":"贪婪匹配","B":"懒惰匹配","C":"占有匹配","D":"随机匹配"}, "a":"B", "desc":""},
  {"id":44, "type":"单选题", "q":"正则表达式[a-z].*3可以配置abc3abc3a3几次（）。", "opts":{"A":"0","B":"1","C":"2","D":"3"}, "a":"B", "desc":"默认为贪婪匹配，.* 会尽可能多地匹配，直接匹配到最后一个3，所以是一次。"},
  {"id":45, "type":"单选题", "q":"下列选项中，表示匹配的字符串开头元字符是（）。", "opts":{"A":"*","B":"^","C":"$","D":"[]"}, "a":"B", "desc":""},
  {"id":46, "type":"单选题", "q":"能将'baidu_logo.png'正确的保存到文件中的代码是（）。", "opts":{"A":"with open('baidu_logo.png', 'wb') as file:","B":"with open('baidu_logo.png', 'w') as file:","C":"with open('baidu_logo.png', 'wr') as file:","D":"with open('baidu_logo.png', 'a+') as file:"}, "a":"A", "desc":"图片是二进制数据，必须使用 'wb' (write binary) 模式。"},
  {"id":47, "type":"单选题", "q":"下列选项中，BeautifulSoup使用CSS选择器的方法是（）。", "opts":{"A":"search()","B":"findall()","C":"find()","D":"select()"}, "a":"D", "desc":""},
  {"id":48, "type":"单选题", "q":"使用xpath获取文本使用（）。", "opts":{"A":"text","B":"text()","C":"content","D":"content()"}, "a":"B", "desc":"在XPath中使用 /text() 获取节点内容。"},
  {"id":49, "type":"单选题", "q":"下列XPath路径表达式中。用于选取第一个app元素的是（）。", "opts":{"A":"/appstore/app(1)","B":"/appstore/app(first)","C":"/appstore/app[1]","D":"/appstore/app[first]"}, "a":"C", "desc":"XPath 索引从 1 开始。"},
  {"id":50, "type":"单选题", "q":"下列选项中，属于Selenium访问指定URL地址的方法是（）。", "opts":{"A":"get()","B":"post()","C":"head()","D":"put()"}, "a":"A", "desc":""},
  {"id":51, "type":"单选题", "q":"下列选项中，关于Selenium的描述说法错误的是（）。", "opts":{"A":"Selenium是一个开源的、便携式的自动化测试工具","B":"Selenium可以模拟用户使用浏览器完成一些动作","C":"Selenium最初的目的是为了便于网络爬虫抓取动态网页数据","D":"Selenium需要通过浏览器驱动程序WebDriver才能与所选浏览器进行交互"}, "a":"C", "desc":"Selenium最初是作为Web应用自动化测试工具开发的，后来才被用于爬虫。"},
  {"id":52, "type":"单选题", "q":"关于jsonpath模块的描述，说法错误的是（）。", "opts":{"A":"jsonpath是一个解析JSON文档的模块","B":"jsonpath()函数根据JSONPath的表达式定位目标对象","C":"jsonpath函数会返回包含解析后的结果的列表","D":"jsonpath模块可以解析XML文档中的数据"}, "a":"D", "desc":"jsonpath 只能解析 JSON。"},
  {"id":53, "type":"单选题", "q":"下列选项中，关于网络爬虫合法性探究的描述说法错误的是（）。", "opts":{"A":"Robots协议又称爬虫协议","B":"Robots协议能够有效防范网络爬虫","C":"爬虫会给网站增加不小的压力","D":"Robots协议没有实际的约束力"}, "a":"B", "desc":"Robots协议没有强制技术约束力，防君子不防小人。"},
  {"id":54, "type":"单选题", "q":"以下关于列表操作的描述,错误的是（）。", "opts":{"A":"通过 append 方法可以向列表添加元素","B":"通过 extend 方法可以将另一个列表中的元素逐一添加到列表中","C":"通过insert(index, object)方法,在指定位置index前插入元素","D":"通过 add 方法可以向列表添加元素"}, "a":"D", "desc":"Python列表没有 add 方法，add 是集合(set)的方法。"},
  {"id":55, "type":"单选题", "q":"下列选项中，关于设置代理服务器的描述错误的是（）。", "opts":{"A":"降低单个IP访问频率","B":"防止IP被封禁","C":"加快访问网站的速度","D":"代理IP的寿命是有限的"}, "a":"C", "desc":"使用代理通常会增加中转时间，速度不一定加快。"},
  {"id":56, "type":"单选题", "q":"以下 Python 语言关键字在异常处理结构中用来捕获特定类型异常的选项是（）。", "opts":{"A":"for","B":"lambda","C":"in","D":"except"}, "a":"D", "desc":""},
  {"id":57, "type":"单选题", "q":"在匹配嵌套了HTML内容的文本时，会忽略HTML内容本身存在的层次结构的解析语言是（）。", "opts":{"A":"正则表达式","B":"XPath","C":"Beautiful Soup","D":"所有选项均正确"}, "a":"A", "desc":""},
  {"id":58, "type":"单选题", "q":"demo_dict = {\"city\": \"北京\", \"name\": \"小明\"}。print(json.dumps(demo_dict, ensure_ascii=False)) 运行结果为（）。", "opts":{"A":"{\"city\": \"北京\", \"name\": \"小明\"}","B":"{\"city\": \"\\u5317\\u4eac\", \"name\": \"\\u5c0f\\u660e\"}","C":"{}","D":"运行错误"}, "a":"A", "desc":"ensure_ascii=False 确保中文正常输出。"},
  {"id":59, "type":"单选题", "q":"https的端口号是（）。", "opts":{"A":"80","B":"8080","C":"443","D":"433"}, "a":"C", "desc":""},
  {"id":60, "type":"单选题", "q":"下列哪个正则表达式与 1\\d{5,9}不相同（）。", "opts":{"A":"[1]\\d{5,9}","B":"1[0-9]{5,9}","C":"1[0123456789]{5,9}","D":"[1]\\D{5,9}"}, "a":"D", "desc":"\\D 匹配非数字，与 \\d 相反。"},
  {"id":61, "type":"单选题", "q":"下列不能匹配任意字符的正则表达式是（）。", "opts":{"A":"[\\d\\D]","B":"[\\w\\W]","C":"[\\s\\S]","D":"[\\a\\A]"}, "a":"D", "desc":"\\a 不是标准元字符。"},
  {"id":62, "type":"单选题", "q":"下列选项中，表示向服务器提交表单或上传文件的请求方法是（）。", "opts":{"A":"GET","B":"POST","C":"HEAD","D":"PUT"}, "a":"B", "desc":""},
  {"id":63, "type":"单选题", "q":"下列选项中，用于以二进制形式获取响应内容的属性是（）。", "opts":{"A":"status_code","B":"text","C":"content","D":"string"}, "a":"C", "desc":""},
  {"id":64, "type":"单选题", "q":"bs4中，若已找到节点并存放于变量x中，能获取节点内容的是（）。", "opts":{"A":"x.text","B":"x.content","C":"x.html","D":"x.attrs"}, "a":"A", "desc":""},
  {"id":65, "type":"单选题", "q":"以下XPath谓语中，能获得满足条件的第一个节点的是（）。", "opts":{"A":"[0]","B":"[1]","C":"[first()]","D":"[min()+1]"}, "a":"B", "desc":"XPath 位置索引从 1 开始。"},
  {"id":66, "type":"单选题", "q":"lxml库中，用于解析xml文件的方法是（）。", "opts":{"A":"etree.parse()","B":"etree.XML()","C":"etree.HTML()","D":"etree.fromstring()"}, "a":"A", "desc":"parse() 用于解析文件。"},
  {"id":67, "type":"单选题", "q":"关于JSONPath的描述，说法错误的是（）。", "opts":{"A":"JSONPath只适用于JSON文档","B":"JSONPath提供了描述JSON文档层次结构的表达式","C":"JSONPath提供的语法与XPath提供的语法相同","D":"JSONPath可以看作定位目标对象位置的语言"}, "a":"C", "desc":"JSONPath 借鉴了 XPath，但语法不完全相同（例如根节点用 $ 而不是 /）。"},
  {"id":68, "type":"单选题", "q":"requests库中，proxies参数传入一个字典，该字典中包含了所需要的代理IP，其中字典的键为（）。", "opts":{"A":"get","B":"ip地址","C":"协议类型（http或https）","D":"post"}, "a":"C", "desc":"例如 {'http': '...'}。"},
  {"id":69, "type":"单选题", "q":"URL地址'https://www.baidu.com?ie=utf-8&wd=python'，其中属于表示查询字符串的是（）。", "opts":{"A":"ie=utf-8&wd=python","B":"https","C":"www.baidu.com","D":"wd=python"}, "a":"A", "desc":""},
  {"id":70, "type":"单选题", "q":"关于CSS选择器的描述，说法错误的是（）。", "opts":{"A":"类别选择器是根据类名选择元素，类名前面用“.”进行标注","B":"ID选择器是根据特定ID选择元素，ID前面加上“$”进行标注","C":"属性选择器是根据元素的属性选择元素，属性必须用中括号进行包裹","D":"元素选择器是根据元素名称选择元素"}, "a":"B", "desc":"ID选择器使用 #，不是 $。"},
  {"id":71, "type":"单选题", "q":"以下选项中是HTTP请求行的是（）。", "opts":{"A":"GET / HTTP/1.1","B":"Connection: keep-alive","C":"Accept-Language: zh-CN,zh;q=0.9","D":"User-Agent: Mozilla/5.0 ..."}, "a":"A", "desc":""},
  {"id":72, "type":"单选题", "q":"GET请求方法通过请求参数传输数据，最多能传输的数据量是（）。", "opts":{"A":"2KB","B":"4KB","C":"1M","D":"无限制"}, "a":"A", "desc":"浏览器一般限制在 2KB 左右。"},
  {"id":73, "type":"单选题", "q":"关于Beautiful Soup的描述，说法错误的是（）。", "opts":{"A":"Beautiful Soup是一个用于从HTML或XML文档中提取目标数据的Python库","B":"Beautiful Soup支持CSS选择器","C":"Beautiful Soup可以将HTML或XML文档、片段转换成节点树","D":"Beautiful Soup会将整个节点树看作一个Python类的对象"}, "a":"D", "desc":"它将节点树中的**每个节点**看作Python对象。"},
  {"id":74, "type":"单选题", "q":"selenium中，浏览器对象往网页的输入框中输入文字需要调用的方法是（）。", "opts":{"A":"get()","B":"save_screenshot()","C":"send_keys()","D":"find_element_by_id()"}, "a":"C", "desc":""},
  {"id":75, "type":"单选题", "q":"selenium中，用于关闭浏览器对象的是（）。", "opts":{"A":"driver.cancel()","B":"driver.exit()","C":"driver.quit()","D":"driver.close()"}, "a":"C", "desc":""},
  {"id":76, "type":"单选题", "q":"selenium中，能查找 <form name='hello'></form> 的元素的表达式是（）。", "opts":{"A":"find_element_by_css_selector('hello')","B":"find_element_by_class_name('hello')","C":"find_element_by_tag_name('hello')","D":"find_element_by_name('hello')"}, "a":"D", "desc":""},
  {"id":77, "type":"单选题", "q":"下列选项中，关于进程的描述错误的是( )。", "opts":{"A":"进程是系统进行资源分配的最小单位","B":"进程拥有自己的内存空间","C":"进程之间数据不共享","D":"进程的存在必须依赖线程"}, "a":"D", "desc":"进程不依赖线程，早期操作系统只有进程。"},
  {"id":78, "type":"单选题", "q":"下列选项中，关于多线程爬虫的述错误的是( )。", "opts":{"A":"开启的线程数量越多，程序运行速度越快","B":"多线程爬虫可以有效利用CPU和网络I/O等待时间，提高数据采集效率","C":"多线程爬虫使用队列是为了保证安全地使用多线程采集网页数据","D":"通常情况下，多程爬虫会开启多个线程抓取网页和解析网页"}, "a":"A", "desc":"线程过多会导致上下文切换开销变大，速度反而变慢。"},
  {"id":79, "type":"单选题", "q":"以下哪个是多线程爬虫中，用于等待子线程结束的方法？( )", "opts":{"A":"start()","B":"join()","C":"setDaemon()","D":"run()"}, "a":"B", "desc":""},
  {"id":80, "type":"单选题", "q":"在Python中，以下哪个选项用于创建一个线程对象？( )", "opts":{"A":"threading.create_thread()","B":"threading.Thread()","C":"threading.new_thread()","D":"threading.start_thread()"}, "a":"B", "desc":""},
  {"id":81, "type":"单选题", "q":"在Python中，通过继承Thread类创建自定义线程时，需要重写哪个方法？( )", "opts":{"A":"__init__()","B":"start()","C":"run()","D":"execute()"}, "a":"C", "desc":""},
  {"id":82, "type":"单选题", "q":"下列选项中，关于Scrapy框架的描述正确的是（ ）。", "opts":{"A":"Scrapy是一个纯使用Python语言开发的收费的网络爬虫框架","B":"Scrapy支持Shell工具，方便开发人员独立调试程序","C":"Scrapy自身可以实现分布式爬虫","D":"Scrapy是基于Scrapy-Splash框架开发的"}, "a":"B", "desc":"A错在“收费”；C错在自身不支持分布式（需scrapy-redis）；D错在因果倒置。"},
  {"id":83, "type":"判断题", "q":"爬虫的合法性完全取决于网站的robots.txt文件。", "a":"错", "desc":"还取决于法律法规（如个人信息保护法）。"},
  {"id":84, "type":"判断题", "q":"互联网上每个文件都有一个唯一的URL。", "a":"对", "desc":""},
  {"id":85, "type":"判断题", "q":"一个IP地址只能对应一个域名。", "a":"错", "desc":"一个IP可以对应多个域名（共享主机）。"},
  {"id":86, "type":"判断题", "q":"HTTP协议是无状态的，这意味着每次请求都是独立的，不会记住之前的请求信息。", "a":"对", "desc":""},
  {"id":87, "type":"判断题", "q":"爬虫在抓取动态内容时，如果直接使用requests库获取网页源码，能够获得所有渲染后的内容。", "a":"错", "desc":"Requests 只能获取服务器原始响应，不能执行 JS。"},
  {"id":88, "type":"判断题", "q":"get方法比post方法速度更快。", "a":"对", "desc":"一般情况下 GET 比 POST 快。"},
  {"id":89, "type":"判断题", "q":"一次HTTP通信的过程包括HTTP请求和HTTP响应。", "a":"对", "desc":""},
  {"id":90, "type":"判断题", "q":"在HTTP请求中，GET方法比POST方法更适合传输大量数据。", "a":"错", "desc":"POST 更适合。"},
  {"id":91, "type":"判断题", "q":"在HTTP请求头中，Referer字段会告知服务器请求的来源页面，能帮助网站分析流量来源。", "a":"对", "desc":""},
  {"id":92, "type":"判断题", "q":"当服务器返回HTTP状态码404时，表示请求的资源没有找到。", "a":"对", "desc":""},
  {"id":93, "type":"判断题", "q":"爬虫可以通过模拟点击事件来动态获取数据，但如果没有正确处理JavaScript渲染，它仍然无法抓取数据。", "a":"对", "desc":""},
  {"id":94, "type":"判断题", "q":"Python中的open()函数可以用于打开网页并读取网页内容。", "a":"错", "desc":"open() 只能打开本地文件，需用 requests/urllib 打开网页。"},
  {"id":95, "type":"判断题", "q":"requests.get()方法可以用来发送GET请求并返回一个包含响应内容的对象。", "a":"对", "desc":""},
  {"id":96, "type":"判断题", "q":"在XPath中，//input[@type='text']可以选择所有type='text'的input元素，无论其层级如何。", "a":"对", "desc":""},
  {"id":97, "type":"判断题", "q":"在XPath中，//a[starts-with(@href, 'https')]会选择所有href属性以https开头的a标签。", "a":"对", "desc":""},
  {"id":98, "type":"判断题", "q":"在XPath中，//div[@id='content']/text()可以选取id='content'的div标签中的所有文本节点。", "a":"错", "desc":"只能选取直接子节点的文本，不能选取后代节点的文本。"},
  {"id":99, "type":"判断题", "q":"XPath表达式//div[@class='header'][contains(text(),'Python')] 会选择class='header'且包含文本Python的div标签。", "a":"对", "desc":""},
  {"id":100, "type":"判断题", "q":"在XPath中，//div[@class='header']/*[2]会选择class='header'的div标签下的第二个子元素，不管它是什么类型的标签。", "a":"对", "desc":""},
  {"id":101, "type":"判断题", "q":"soup.select('div p#main') 会选择所有 div 标签中的 id='main' 的 p 标签。", "a":"对", "desc":""},
  {"id":102, "type":"判断题", "q":"soup.select('div .header > p') 会选择所有类名为 header 的div元素的直接子元素 p 。", "a":"错", "desc":"div .header 表示 div 的后代中类名为 header 的元素，而不是 div 元素的类名为 header。"},
  {"id":103, "type":"判断题", "q":"soup.select('div#header')用于选取页面中第一个id='header'的div标签。", "a":"错", "desc":"select() 返回列表，包含所有符合条件的元素。"},
  {"id":104, "type":"判断题", "q":"在BeautifulSoup中，soup.find_all('div', {'class': 'header'}) 与soup.find_all('div', class_='header')效果是相同的。", "a":"对", "desc":""},
  {"id":105, "type":"判断题", "q":"在BeautifulSoup中，.get_text()方法会返回标签内所有文本内容，但不包括任何子标签的内容。", "a":"错", "desc":"包括子标签内容。"},
  {"id":106, "type":"判断题", "q":"soup.find('p', {'class': 'intro'}).find('a') 会选取class='intro'的p标签下第一个a标签。", "a":"对", "desc":""},
  {"id":107, "type":"判断题", "q":"soup.find('div', class_='header').find_all('p') 可以选取class='header'的div标签下所有p标签。", "a":"对", "desc":""},
  {"id":108, "type":"判断题", "q":"在BeautifulSoup中，soup.find_all('div', class_='header')[1] 会返回所有class='header'的div标签中的第一个元素。", "a":"错", "desc":"[1] 是第二个元素。"},
  {"id":109, "type":"判断题", "q":"在BeautifulSoup中，soup.select('div#content p')与soup.find_all('p', {'class': 'content'})效果相同。", "a":"错", "desc":"前者是ID为content的div下的p，后者是class为content的p。"},
  {"id":110, "type":"判断题", "q":"Selenium不支持浏览器的功能，它不需要与第三方浏览器结合使用。", "a":"错", "desc":"必须配合浏览器驱动（WebDriver）。"},
  {"id":111, "type":"判断题", "q":"表层网页是指传统搜索引擎可以索引的页面，主要以超链接可以到达的静态网页构成的网页。", "a":"对", "desc":""},
  {"id":112, "type":"判断题", "q":"POST请求的请求参数会暴露在URL地址中。", "a":"错", "desc":"GET 才会。"},
  {"id":113, "type":"判断题", "q":"JSON比XML的语法更简单，层次结构更加清晰，易于阅读。", "a":"对", "desc":""},
  {"id":114, "type":"判断题", "q":"使用Selenium可以抓取动态网页中的数据。", "a":"对", "desc":""},
  {"id":115, "type":"判断题", "q":"Requests是基于urllib3编写的库。", "a":"对", "desc":""},
  {"id":116, "type":"判断题", "q":"服务器端可以记住用户的登录状态，因此HTTP协议自身具有保持会话状态的功能。", "a":"错", "desc":"HTTP 是无状态协议，通过 Cookie/Session 实现会话。"},
  {"id":117, "type":"判断题", "q":"Selenium启动浏览器后，浏览器的窗口默认以最大化的形式显示。", "a":"错", "desc":"默认不是最大化。"},
  {"id":118, "type":"判断题", "q":"JSONPath只能解析JSON格式的数据。", "a":"对", "desc":""},
  {"id":119, "type":"判断题", "q":"re模块在提取HTML标签中的特定信息时比BeautifulSoup 或lxml会更高效。", "a":"错", "desc":"正则处理复杂 HTML 容易出错且效率不一定高，lxml 通常更快。"},
  {"id":120, "type":"判断题", "q":"POST请求方法的参数信息会在URL地址中显示。", "a":"错", "desc":""},
  {"id":121, "type":"判断题", "q":"在XPath中，@*表示选取所有属性，而//*表示选取所有节点。", "a":"对", "desc":""},
  {"id":122, "type":"判断题", "q":"HTTPS协议在传输数据过程中比HTTP协议更加安全。", "a":"对", "desc":""},
  {"id":123, "type":"判断题", "q":"JSONPath表达式 $..* 可以用来获取JSON对象中所有字段的值。", "a":"对", "desc":""},
  {"id":124, "type":"判断题", "q":"Python中可以使用open()函数将图片数据写入磁盘，其操作模式为'w'。", "a":"错", "desc":"需使用 'wb' (write binary)。"},
  {"id":125, "type":"判断题", "q":"Robots协议可以从根本上约束爬虫程序。", "a":"错", "desc":"只是君子协定。"},
  {"id":126, "type":"判断题", "q":"get方法相比post方法能携带更多信息。", "a":"错", "desc":"POST 载荷更大。"},
  {"id":127, "type":"判断题", "q":"XPath中，路径表达式是唯一的。", "a":"错", "desc":"同一节点可有多种路径写法。"},
  {"id":128, "type":"判断题", "q":"CSS用于向网页中添加交互行为。", "a":"错", "desc":"JS 用于交互，CSS 用于样式。"},
  {"id":129, "type":"判断题", "q":"如果服务器返回的状态码为500，则表示客户端发送的请求出现错误。", "a":"错", "desc":"5xx 是服务器错误，4xx 是客户端错误。"},
  {"id":130, "type":"判断题", "q":"在使用Selenium时，.get()方法可以加载页面并等待页面完全加载后返回控制权，但不能用于抓取JavaScript渲染的数据。", "a":"错", "desc":"它可以抓取渲染后的数据。"},
  {"id":131, "type":"判断题", "q":"在实际应用中，文件存储和数据库存储各有利弊，文件存储比较适合中小型网络爬虫，数据库存储比较适合大型网络爬虫。", "a":"对", "desc":""},
  {"id":132, "type":"判断题", "q":"线程具有独立运行、状态不可测、执行顺序随机的特点。", "a":"对", "desc":""},
  {"id":133, "type":"判断题", "q":"线程是系统进行资源分配的最小单位。", "a":"错", "desc":"进程是资源分配单位，线程是调度单位。"},
  {"id":134, "type":"判断题", "q":"线程共享同一进程中的数据。", "a":"对", "desc":""},
  {"id":135, "type":"判断题", "q":"一个Scrapy项目可以包含多个爬虫。", "a":"对", "desc":""},
  {"id":136, "type":"简答题", "q":"请列举并阐述网络爬虫的常见分类。", "a":"通用网络爬虫：又称全网爬虫，是指访问全互联网资源的网络爬虫。\n聚焦网络爬虫：又称主题网络爬虫，是指有选择性地访问那些与预定主题相关网页的网络爬虫。\n增量式网络爬虫：是指对已下载的网页采取增量式更新，只抓取新产生或者已经发生变化的网页的网络爬虫。\n深层网络爬虫：是指抓取深层网页的网络爬虫，它要抓取的网页层次比较深，需要通过一定的附加策略才能够自动抓取。", "desc":""},
  {"id":137, "type":"简答题", "q":"XPATH路径表达式由哪些符号组成，请列举并描述它们的含义。", "a":"/ :表示从当前节点选择直接子节点。\n// :表示从当前节点选择子孙节点，忽略层级关系。\n. :表示选取当前节点。\n.. :表示选取当前节点的父节点。\n@ :选取属性节点。\n[] :表示谓词", "desc":""},
  {"id":138, "type":"简答题", "q":"请简述浏览器访问百度首页的加载过程。", "a":"1. 浏览器通过DNS服务器查找百度服务器对应的IP地址；\n2. 浏览器向IP地址对应的Web服务器发送HTTP请求；\n3. Web服务器接收HTTP请求后进行处理，向浏览器返回HTML页面；\n4. 浏览器对HTML页面进行渲染呈现给用户。", "desc":""},
  {"id":139, "type":"简答题", "q":"请简要说明什么是Cookie，它有什么用途？", "a":"Cookie 是在用户计算机上存储小型文本文件的技术。当用户访问网站时，网站将Cookie发送到用户浏览器存储。之后再次访问时，浏览器会发送Cookie给服务器。用途：识别用户、跟踪会话状态（如登录状态）。", "desc":""},
  {"id":140, "type":"简答题", "q":"请求方法GET和POST有哪些主要区别。", "a":"1. GET用于请求资源，POST用于提交数据。\n2. GET传输数据量小（约2KB），POST无限制。\n3. GET参数在URL中明文显示，安全性低；POST在请求体中，相对安全。", "desc":""},
  {"id":141, "type":"简答题", "q":"常见的反爬虫措施有哪些，请列举并简述应对方法。", "a":"1. User-Agent检测：添加User-Agent伪装成浏览器。\n2. 封IP：使用代理IP或降低抓取频率。\n3. 验证码：使用图像识别或打码平台。\n4. 动态加载：使用Selenium或分析Ajax请求。", "desc":""},
  {"id":142, "type":"简答题", "q":"请简述使用requests库进行网页数据抓取的一般流程。", "a":"1. 分析网页，确定目标URL。\n2. 确定请求方式和Header，使用requests发送请求。\n3. 获取响应内容（源码）。\n4. 解析源码提取数据。\n5. 存储数据。", "desc":""},
  {"id":143, "type":"简答题", "q":"请列举4种用selenium定位单个元素的方法并进行简单说明。", "a":"By.ID: 通过id属性定位\nBy.XPATH: 通过XPath路径定位\nBy.CSS_SELECTOR: 通过CSS选择器定位\nBy.TAG_NAME: 通过标签名定位\nBy.NAME: 通过name属性定位", "desc":""},
  {"id":144, "type":"简答题", "q":"请简述robots的作用，并列举和阐述robots.txt文件中的选项。", "a":"Robots协议用于告知爬虫哪些页面可以抓取。选项：\nUser-agent: 适用爬虫名称\nDisallow: 不允许访问的路径\nAllow: 允许访问的路径\nSitemap: 网站地图路径", "desc":""},
  {"id":145, "type":"简答题", "q":"常见的CSS选择器有哪些，请举例说明。", "a":"类别选择器: .class\nID选择器: #id\n属性选择器: [attr=value]\n后代选择器: div p\n子代选择器: div > p", "desc":""},
  {"id":146, "type":"简答题", "q":"请简述多线程爬虫的运行流程。", "a":"1. 构建网址队列。\n2. 开启多线程抓取网页源码。\n3. 将源码存入队列。\n4. 开启多线程解析源码。\n5. 将数据存入数据队列。\n6. 存储数据。", "desc":""},
  {"id":147, "type":"简答题", "q":"请简述文件存储和数据库存储的特点。", "a":"文件存储：简单，适合中小型数据，本地存储。\n数据库存储：支持分类、去重、高效检索，适合大型数据。", "desc":""},
  {"id":148, "type":"简答题", "q":"请简述Scrapy框架的优点和缺点。", "a":"优点：文档丰富、并发强、XPath解析快、有中间件、支持Shell调试、扩展性强。\n缺点：不支持分布式（需插件）、去重消耗内存、无法直接处理JS动态页面。", "desc":""},
  {"id":149, "type":"操作题", "q":"编写一个Python程序，使用requests模块发送一个GET请求到http://httpbin.org/get，并打印出相应的text内容。", "a":"import requests\nresponse = requests.get('http://httpbin.org/get')\nprint(response.text)", "desc":""},
  {"id":150, "type":"操作题", "q":"编写一个Python程序，将一个包含数据的字典写入output.json文件中，并确保输出的JSON字符串是格式化的(缩进为4个单位)。\ndata = {\"title\": \"Sample Data\"...}", "a":"import json\n\ndata = {\n    \"title\": \"Sample Data\",\n    \"author\": \"Charlie\",\n    \"publication_year\": 2022,\n    \"genres\": [\"Fiction\", \"Adventure\"]\n}\n\nwith open('output.json', 'w') as file:\n    json.dump(data, file, indent=4, sort_keys=True)", "desc":""},
  {"id":151, "type":"操作题", "q":"现有html代码字符串(html变量)，请用XPath提取第一个商品名称、第二个商品价格、第二个商品链接。", "a":"from lxml import etree\ntree = etree.HTML(html)\nproduct1_name = tree.xpath('//div[@class=\"product\"][1]//h2[@class=\"name\"]/text()')[0]\nproduct2_price = tree.xpath('//div[@class=\"product\"][2]//p[@class=\"price\"]/text()')[0]\nproduct2_link = tree.xpath('//div[@class=\"product\"][2]//a/@href')[0]", "desc":""},
  {"id":152, "type":"操作题", "q":"接上题，请使用BeautifulSoup实现同样的功能。", "a":"from bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nproduct1_name = soup.select('.product > .name')[0].text\nproduct2_price = soup.select('.product > .price')[1].text\nproduct2_link = soup.select('.product > a')[1].get('href')", "desc":""},
  {"id":153, "type":"操作题", "q":"现有html表格代码，请使用bs4获取排名、学校中文名称、省份、类型信息。", "a":"from bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nrank = soup.select('.rank')[0].text\nname = soup.select('.name.cn')[0].text\naddress = soup.select('#address')[0].text\ncategory = soup.select('span')[0].text\nprint(rank, name, address, category)", "desc":""},
  {"id":154, "type":"操作题", "q":"接上题，请使用lxml(XPath)实现同样的功能。", "a":"from lxml import etree\ntree = etree.HTML(html)\nrank = tree.xpath('//td[@class=\"rank\"]/text()')[0]\nname = tree.xpath('//a[@class=\"name cn\"]/text()')[0]\naddress = tree.xpath('//td[@id=\"address\"]/text()')[0]\ncategory = tree.xpath('//span/text()')[0]\nprint(rank, name, address, category)", "desc":""},
  {"id":155, "type":"操作题", "q":"现有HTML页面源代码html_content，请使用xpath: 1)创建节点树 2)提取标题 3)提取描述 4)提取所有链接href。", "a":"from lxml import etree\ntree = etree.HTML(html_content)\n# 2\ntitle = tree.xpath('//title/text()')[0]\nprint(f\"页面标题:{title}\")\n# 3\ndescription = tree.xpath('//p[@class=\"description\"]/text()')[0]\nprint(f\"页面描述:{description}\")\n# 4\nlinks = tree.xpath('//a/@href')\nfor link in links:\n    print(f\"链接:{link}\")", "desc":""},
  {"id":156, "type":"操作题", "q":"编写Selenium脚本:打开http://www.example.com，定位id='input_id'输入框并输入'Hello, Selenium!'，然后点击id='button_id'的按钮。", "a":"from selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome()\ntry:\n    driver.get(\"http://www.example.com\")\n    input_element = driver.find_element(By.ID, \"input_id\")\n    input_element.send_keys(\"Hello, Selenium!\")\n    button_element = driver.find_element(By.ID, \"button_id\")\n    button_element.click()\nfinally:\n    driver.quit()", "desc":""},
  {"id":157, "type":"操作题", "q":"目标网站共50页，爬取html并提取楼盘名称、地址、面积、房价。请补全参考代码中的download函数部分。", "a":"def download(text):\n    tree = etree.HTML(text)\n    name_list = tree.xpath('//li[@class=\"list\"]/a/img/@alt')\n    address_list = tree.xpath('//div[@class=\"address\"]/text()')\n    size_list = tree.xpath('//div[@class=\"size\"]/text()')\n    price_list = tree.xpath('//div[@class=\"msg_box\"]/div/span/text()')\n    result_list = []\n    for i in range(len(name_list)):\n        name = name_list[i]\n        address = address_list[i]\n        size = size_list[i]\n        price = price_list[i]\n        result_list.append([name, address, size, price])", "desc":""},
  {"id":158, "type":"操作题", "q":"编写爬虫，爬取新浪新闻头条的标题、时间和链接，写入excel。请补全parse_html函数的逻辑。", "a":"def parse_html(text):\n    item = []\n    tree = etree.HTML(text)\n    title_list = tree.xpath('//h2[@class=\"news-title\"]/text()')\n    time_list = tree.xpath('//p[@class=\"news-time\"]/text()')\n    link_list = tree.xpath('//a/@href')\n    for i in range(len(title_list)):\n        title = title_list[i]\n        time = time_list[i]\n        link = link_list[i]\n        item.append({\n            '新闻标题': title,\n            '发布时间': time,\n            '新闻链接': link\n        })\n    return item", "desc":""},
  {"id":159, "type":"操作题", "q":"爬取站长素材网风景图片标题和链接，保存到Excel。请补全parse_html函数。", "a":"def parse_html(text, item):\n    tree = etree.HTML(text)\n    title_list = tree.xpath('//div[@class=\"item\"]/img/@alt')\n    src_list = tree.xpath('//div[@class=\"item\"]/img/@src')\n    for i in range(len(title_list)):\n        title = title_list[i]\n        src = src_list[i]\n        item.append({\n            'Image Title': title,\n            'Image URL': src\n        })", "desc":""},
  {"id":160, "type":"简答题", "q":"请论述在网络爬虫开发中需要考虑哪些法律与伦理问题？作为一名开发者，应如何遵守相关规范？", "a":"法律上：不得侵犯著作权、个人信息及商业秘密，严禁非法侵入系统或扰乱网站运营。\n伦理上：尊重网站意愿（Robots协议）、节约网络资源（控制频率），保证数据使用透明公正。\n作为开发者：事前合规审查，技术上友好爬取、最小化收集数据并妥善保护，事后持续监控行为。", "desc":""},
  {"id":161, "type":"简答题", "q":"请简述你写网络爬虫程序的基本思路或流程，并举例说明爬虫技术在实际应用中的几种常见场景。", "a":"流程：抓包分析 -> 获取静态页面(Requests) -> 应对反爬(User-Agent/IP代理) -> HTML解析(XPath/BS4) -> 数据存储。\n应用场景：搜索引擎索引、电商价格监控、舆情分析。", "desc":""},
  {"id":162, "type":"简答题", "q":"这学期学了哪些解析网页数据的技术？请列举并对不同技术的应用场景或者优缺点进行简单分析说明。", "a":"XPath: 语法通用，解析速度快，适合结构化文档。\nBeautifulSoup: 容错率高，API简单易用，但速度稍慢，适合处理不规范HTML。\nJSONPath: 专门解析JSON数据，语法简洁。\n正则表达式: 灵活强大，适合提取特定模式的字符串，但编写复杂，容易出错。", "desc":""},
  {"id":163, "type":"简答题", "q":"请探讨requests和selenium的技术特性差异，并各举一个典型应用场景。", "a":"Requests: 直接发送HTTP请求，速度快，资源消耗少，但无法处理JS渲染的动态内容。适合抓取静态网页或API接口。\nSelenium: 模拟真实浏览器操作，能处理JS渲染和复杂交互，但速度慢，资源消耗大。适合抓取动态加载、需要登录验证的复杂网页。", "desc":""}
]
