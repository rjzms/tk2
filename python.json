[
  {"id":1, "type":"单选题", "q":"下列选项中,不属于Python开发网络爬虫优势的是（）。", "opts":{"A":"语法简洁,容易上手","B":"开发效率高","C":"丰富的模块","D":"运行速度快、性能强"}, "a":"D", "desc":""},
  {"id":2, "type":"单选题", "q":"下列选项中,被称为主题网络爬虫的是（）。", "opts":{"A":"增量式网络爬虫","B":"通用网络爬虫","C":"深层爬虫","D":"聚焦网络爬虫"}, "a":"D", "desc":""},
  {"id":3, "type":"单选题", "q":"下列选项中,不属于防爬虫策略的是（）。", "opts":{"A":"添加User-agent字段","B":"降低访问频率","C":"反复使用同一IP抓取数据","D":"识别验证码"}, "a":"C", "desc":""},
  {"id":4, "type":"单选题", "q":"\"发布人:张三发布时间:2022-11-18 来源:图情信息中心\".split(' ')的执行结果是（）。", "opts":{"A":"['发布人:张三','','发布时间:2022-11-18','','来源:图情信息中心']","B":"['发布人:张三','发布时间:2022-11-18', '来源:图情信息中心']","C":"('发布人:张三','','发布时间:2022-11-18', '', '来源:图情信息中心')","D":"('发布人:张三','发布时间:2022-11-18','来源:图情信息中心')"}, "a":"B", "desc":""},
  {"id":5, "type":"单选题", "q":"关于浏览器加载网页过程的说法,下列描述错误的是（）。", "opts":{"A":"浏览器通过DNS服务器查找被访问服务器对应的IP地址","B":"浏览器向DNS服务器解析的IP地址发送HTTP请求","C":"Web服务器将响应的HTML页面返回给DNS服务器","D":"浏览器会对HTML页面进行渲染并呈现给用户"}, "a":"C", "desc":"Web服务器应将响应返回给浏览器，而不是DNS服务器。"},
  {"id":6, "type":"单选题", "q":"下列选项中,关于网络爬虫实现技术的描述错误的是（）。", "opts":{"A":"只有Python语言能够实现爬虫程序","B":"使用Python开发网络爬虫程序效率相对其他语言更高","C":"使用C++语言开发网络爬虫程序代码成型速度慢","D":"Java提供了众多解析网页的技术,对网页解析有着良好的支持"}, "a":"A", "desc":"任何支持网络请求的语言都可以写爬虫，不限于Python。"},
  {"id":7, "type":"单选题", "q":"下列选项中,用于解析域名的协议是（）。", "opts":{"A":"HTTP","B":"DNS","C":"FTP","D":"SMTP"}, "a":"B", "desc":""},
  {"id":8, "type":"单选题", "q":"下列选项中,表示超文本传输协议的是（）。", "opts":{"A":"File","B":"HTTP","C":"FTP","D":"Mailto"}, "a":"B", "desc":""},
  {"id":9, "type":"单选题", "q":"下列响应头中,用于告诉客户端资源文件的类型和编码的是（）。", "opts":{"A":"Connection","B":"Content-Encoding","C":"Content-Type","D":"Server"}, "a":"C", "desc":""},
  {"id":10, "type":"单选题", "q":"下列状态码中,表示服务器拒绝访问的是（）。", "opts":{"A":"402","B":"403","C":"404","D":"405"}, "a":"B", "desc":""},
  {"id":11, "type":"单选题", "q":"下列选项中,关于聚焦网络爬虫的描述错误的是（）。", "opts":{"A":"聚焦网络爬虫会随机抓取网页与主题相关的数据","B":"聚焦网络爬虫比通用网络爬虫目的性更强","C":"聚焦网络爬虫会根据一定的网页分析算法对网页进行筛选","D":"聚焦网络爬虫会根据预先设定的主题顺着某个垂直领域进行抓取"}, "a":"A", "desc":"聚焦爬虫是有目的的抓取，不是随机抓取。"},
  {"id":12, "type":"单选题", "q":"下列选项中,在JSONPath中表示选取根对象的是（）。", "opts":{"A":"$","B":"/","C":"@","D":"*"}, "a":"A", "desc":""},
  {"id":13, "type":"单选题", "q":"下列选项中,关于HTTP协议的描述说法错误的是（）。", "opts":{"A":"HTTP协议能够高效准确的传送超文本资源","B":"若协议类型为HTTP,则每次连接可以处理多个请求","C":"HTTP协议中的每个请求都是独立的","D":"HTTP协议用于将Web服务器的超文本资源传送到浏览器中"}, "a":"B", "desc":"HTTP协议是一种一次性连接，限制每次连接只处理一个请求，请求是独立的。"},
  {"id":14, "type":"单选题", "q":"以下哪个选项是文件传输协议,访问共享主机的文件资源（）。", "opts":{"A":"File","B":"FTP","C":"HTTP","D":"Mailto"}, "a":"B", "desc":""},
  {"id":15, "type":"单选题", "q":"下列选项中,关于动态页面的描述说法错误的是（）。", "opts":{"A":"动态网页的内容不一定呈现在网页源代码中","B":"动态网页的访问速度相较于静态网页更快","C":"采用动态网页技术的网站可以实现更多的功能","D":"动态网页相比静态网页,动态网页有数据库支撑"}, "a":"B", "desc":"动态网页通常需要数据库查询和渲染，访问速度通常比静态网页慢。"},
  {"id":16, "type":"单选题", "q":"下列选项中,表示内容类型的字段是（）。", "opts":{"A":"Cache-Control","B":"Connection","C":"Content-Encoding","D":"Content-Type"}, "a":"D", "desc":""},
  {"id":17, "type":"单选题", "q":"下列选项中,用于标识客户端身份的是（）。", "opts":{"A":"HOST","B":"User-Agent","C":"Accept","D":"Referer"}, "a":"B", "desc":""},
  {"id":18, "type":"单选题", "q":"关于响应状态码的描述说法错误的是（）。", "opts":{"A":"响应状态码代表服务器的响应状态","B":"响应状态码的作用是告知客户端请求Web资源的结果","C":"若服务器发生错误,用户便无法获取响应状态码","D":"当响应状态码为200时表示服务器接收请求并成功处理"}, "a":"C", "desc":"服务器发生错误时通常会返回5xx系列的状态码。"},
  {"id":19, "type":"单选题", "q":"下列选项中,关于Requests库post()函数的说法错误的是（）。", "opts":{"A":"如果请求数据类型为Json可通过参数json传递","B":"post()函数会根据传入的URL构建一个请求并将该请求发送给服务器","C":"post()函数通过参数data携带请求数据","D":"post()函数既可以发送GET请求也可以发送POST请求"}, "a":"D", "desc":"post()函数专门用于发送POST请求，get()函数用于发送GET请求。"},
  {"id":20, "type":"单选题", "q":"关于抓取静态网页实现技术的说法,下列描述错误的是（）。", "opts":{"A":"如果要抓取静态网页的数据,只需要获得网页的源代码即可","B":"通过urllib、urllib3和Requests等库抓取静态网页数据","C":"Requests库只能发送网络请求不能获取网页源码","D":"抓取静态网页数据的整个过程是模仿用户通过浏览器访问网页的过程"}, "a":"C", "desc":"Requests库不仅能发送请求，通过response.text或content即可获取源码。"},
  {"id":21, "type":"单选题", "q":"下列选项中,不属于HTML元素组成的是（）。", "opts":{"A":"开始标签","B":"内容","C":"样式","D":"结束标签"}, "a":"C", "desc":""},
  {"id":22, "type":"单选题", "q":"下列选项中,表示图像标签的是（）。", "opts":{"A":"<html>","B":"<h1>","C":"<p>","D":"<img>"}, "a":"D", "desc":""},
  {"id":23, "type":"单选题", "q":"下列选项中,不属于请求行组成的是（）。", "opts":{"A":"请求方法","B":"URL","C":"协议版本","D":"请求数据"}, "a":"D", "desc":"HTTP请求格式中,请求行包括请求方法、URL和协议版本。请求数据在请求体中。"},
  {"id":24, "type":"单选题", "q":"下列选项中,关于静态页面的描述说法错误的是（）。", "opts":{"A":"静态网页的交互性较差,在功能方面有较大的限制","B":"静态网页的访问速度快,访问过程中无需连接数据库","C":"静态网页没有数据库的支持,内容更新与维护比较复杂","D":"静态网页的内容可根据用户信息进行定制化展示"}, "a":"D", "desc":"静态网页内容固定，无法根据用户信息定制化展示，那是动态网页的特性。"},
  {"id":25, "type":"单选题", "q":"下列选项中,关于处理响应的描述说法错误的是（）。", "opts":{"A":"当服务器返回的响应状态码为200时,表明可以接收到由服务器返回的响应信息","B":"Response类的对象中封装了服务器返回的响应信息","C":"响应内容中只能包含文本内容","D":"若想获取响应的最终URL,可通过url属性获取"}, "a":"C", "desc":"响应内容也可以是二进制数据（如图片、视频等）。"},
  {"id":26, "type":"单选题", "q":"下列选项中,用于在GET请求中传递查询字符串的是（）。", "opts":{"A":"params","B":"headers","C":"verify","D":"timeout"}, "a":"A", "desc":""},
  {"id":27, "type":"单选题", "q":"下列选项中,用于查看响应状态码的属性是（）。", "opts":{"A":"content","B":"headers","C":"text","D":"status_code"}, "a":"D", "desc":""},
  {"id":28, "type":"单选题", "q":"下列选项中,关于检测代理IP有效性的描述说法错误的是（）。", "opts":{"A":"当使用代理访问网站时,返回的状态码为200时表示代理可用","B":"当代理无效时,不能返回响应信息","C":"使用的代理IP通过参数proxies传递","D":"post()函数无法使用代理ip"}, "a":"D", "desc":"post()函数同样可以使用proxies参数设置代理IP。"},
  {"id":29, "type":"单选题", "q":"（）会将数据包原封不动地转发给服务器,让服务器认为当前访问的用户只是一个普通客户端,而不是代理服务器。", "opts":{"A":"高度匿名代理服务器","B":"普通匿名代理服务器","C":"透明代理服务器","D":"所有选项均不对"}, "a":"A", "desc":""},
  {"id":30, "type":"单选题", "q":"关于定制请求头的描述说法错误的是（）。", "opts":{"A":"参数headers可以接收列表类型的数据","B":"定制的请求头需要由参数headers中传递","C":"get()函数和post()函数均可以添加定制请求头","D":"定制请求的目的是将发送的请求伪装成浏览器发送的请求"}, "a":"A", "desc":"参数headers只支持字典(dict)类型的值。"},
  {"id":31, "type":"单选题", "q":"阅读代码：response.encoding = 'ISO-8859-1'，print(response.text) 会使用哪种编码方式返回文本（）。", "opts":{"A":"utf-8","B":"gbk","C":"gbk2312","D":"ISO-8859-1"}, "a":"D", "desc":""},
  {"id":32, "type":"单选题", "q":"requests库中,get()函数能用于设置是否启用SSL证书的参数是（）。", "opts":{"A":"url","B":"headers","C":"verify","D":"proxies"}, "a":"C", "desc":""},
  {"id":33, "type":"单选题", "q":"关于Requests库中get()函数的说法错误的是（）。", "opts":{"A":"get()函数既可以发送GET请求也可以发送POST请求","B":"get()函数中参数url是必选参数,该参数含义为请求地址","C":"get()函数会根据传入的URL构建一个请求","D":"使用get()函数发送GET请求时可以携带请求参数"}, "a":"A", "desc":"get()只能发送GET请求。"},
  {"id":34, "type":"单选题", "q":"下列选项中,关于Cookie的描述错误的是（）。", "opts":{"A":"Cookie是一段文本数据,由一个名称和一个值组成","B":"Cookie的生存期可以由开发人员设置","C":"Cookie数据存储在网站服务器中","D":"Cookie是为了网站辨别用户身份、进行会话跟踪而存储的数据"}, "a":"C", "desc":"Cookie数据存储在客户端（浏览器）中。"},
  {"id":35, "type":"单选题", "q":"下列正则表达式中,表示只能匹配任意数字的是（）。", "opts":{"A":"\\w","B":"\\s","C":"\\d","D":"\\b"}, "a":"C", "desc":""},
  {"id":36, "type":"单选题", "q":"下列选项中,表示匹配前导字符0次或1次的是（）。", "opts":{"A":"?","B":"*","C":"+","D":"{n}"}, "a":"A", "desc":""},
  {"id":37, "type":"单选题", "q":"re模块中,对正则表达式进行预编译,从而生成一个代表正则表达式的Pattern对象的方法是（）。", "opts":{"A":"re.pattern()","B":"re.split()","C":"re.run()","D":"re.compile()"}, "a":"D", "desc":""},
  {"id":38, "type":"单选题", "q":"关于正则表达式的描述,说法错误的是（）。", "opts":{"A":"一条正则表达式也称为一个模式","B":"正则表达式匹配HTML时会根据其层次结构进行匹配","C":"正则表达式由普通字符、元字符或预定义字符集组成","D":"正则表达式是对字符串操作的一种逻辑公式"}, "a":"B", "desc":"正则表达式在匹配HTML时会忽略其层次结构，将其作为普通字符串处理。"},
  {"id":39, "type":"单选题", "q":"下列选项中,关于设置代理服务器目的的说法正确的是（）。", "opts":{"A":"加快网络爬虫抓取数据的速度","B":"识别网站验证码","C":"降低访问网站速度","D":"防止IP被封禁"}, "a":"D", "desc":""},
  {"id":40, "type":"单选题", "q":"XPath路径表达式中,在搜索节点时会忽略层级关系的是（）。", "opts":{"A":"/","B":"//","C":"[]","D":"@"}, "a":"B", "desc":""},
  {"id":41, "type":"单选题", "q":"关于XPath的描述,说法错误的是（）。", "opts":{"A":"XPath基于XML或HTML的节点树定位目标节点所在的位置","B":"XPath是一种用于确定XML文档中部分节点位置的语言","C":"XPath匹配节点的方式与正则表达式匹配字符串的方式类似","D":"XPath通过路径表达式可以快速地定位与选取XML或HTML文档中的一个节点或者一组节点集"}, "a":"C", "desc":"XPath是基于树结构的路径选择，正则表达式是基于字符模式的匹配，两者原理不同。"},
  {"id":42, "type":"单选题", "q":"img标签中的什么属性,用于指图片地址（）。", "opts":{"A":"src","B":"href","C":"title","D":"alt"}, "a":"A", "desc":""},
  {"id":43, "type":"单选题", "q":"当正则表达式中包含能接受重复的限定符时,匹配尽可能少的字符,这被称为（）。", "opts":{"A":"贪婪匹配","B":"懒惰匹配","C":"占有匹配","D":"随机匹配"}, "a":"B", "desc":""},
  {"id":44, "type":"单选题", "q":"正则表达式[a-z].*3可以匹配abc3abc3a3几次（）。", "opts":{"A":"0","B":"1","C":"2","D":"3"}, "a":"B", "desc":"默认为贪婪匹配，会一次性匹配到最后一个3。"},
  {"id":45, "type":"单选题", "q":"下列选项中,表示匹配的字符串开头元字符是（）。", "opts":{"A":"*","B":"^","C":"$","D":"."}, "a":"B", "desc":""},
  {"id":46, "type":"单选题", "q":"能将'baidu_logo.png'正确的保存到文件中的代码是（）。", "opts":{"A":"with open('baidu_logo.png', 'wb') as file:","B":"with open('baidu_logo.png', 'w') as file:","C":"with open('baidu_logo.png', 'wr') as file:","D":"with open('baidu_logo.png', 'a+') as file:"}, "a":"A", "desc":"图片是二进制文件，需要使用 'wb' 模式。"},
  {"id":47, "type":"单选题", "q":"下列选项中,BeautifulSoup使用CSS选择器的方法是（）。", "opts":{"A":"search()","B":"findall()","C":"find()","D":"select()"}, "a":"D", "desc":""},
  {"id":48, "type":"单选题", "q":"使用xpath获取文本使用（）。", "opts":{"A":"text","B":"text()","C":"content","D":"content()"}, "a":"B", "desc":"在XPath中使用 text() 函数获取节点文本。"},
  {"id":49, "type":"单选题", "q":"下列XPath路径表达式中，用于选取第一个app元素的是（）。", "opts":{"A":"/appstore/app(1)","B":"/appstore/app(first)","C":"/appstore/app[1]","D":"/appstore/app[first]"}, "a":"C", "desc":"XPath索引从1开始，使用方括号[]。"},
  {"id":50, "type":"单选题", "q":"下列选项中,属于Selenium访问指定URL地址的方法是（）。", "opts":{"A":"get()","B":"post()","C":"head()","D":"put()"}, "a":"A", "desc":""},
  {"id":51, "type":"单选题", "q":"下列选项中,关于Selenium的描述说法错误的是（）。", "opts":{"A":"Selenium是一个开源的、便携式的自动化测试工具","B":"Selenium可以模拟用户使用浏览器完成一些动作","C":"Selenium最初的目的是为了便于网络爬虫抓取动态网页数据","D":"Selenium需要通过浏览器驱动程序WebDriver才能与所选浏览器进行交互"}, "a":"C", "desc":"Selenium最初是作为Web应用自动化测试工具开发的。"},
  {"id":52, "type":"单选题", "q":"关于jsonpath模块的描述,说法错误的是（）。", "opts":{"A":"jsonpath是一个解析JSON文档的模块","B":"jsonpath()函数根据JSONPath的表达式定位目标对象","C":"jsonpath函数会返回包含解析后的结果的列表","D":"jsonpath模块可以解析XML文档中的数据"}, "a":"D", "desc":"jsonpath只能解析JSON，XPath用于解析XML/HTML。"},
  {"id":53, "type":"单选题", "q":"下列选项中,关于网络爬虫合法性探究的描述说法错误的是（）。", "opts":{"A":"Robots协议又称爬虫协议","B":"Robots协议能够有效防范网络爬虫","C":"爬虫会给网站增加不小的压力","D":"Robots协议没有实际的约束力"}, "a":"B", "desc":"Robots协议只是君子协定，没有强制约束力，不能有效物理防范爬虫。"},
  {"id":54, "type":"单选题", "q":"以下关于列表操作的描述,错误的是（）。", "opts":{"A":"通过 append 方法可以向列表添加元素","B":"通过 extend 方法可以将另一个列表中的元素逐一添加到列表中","C":"通过insert(index, object)方法,在指定位置index前插入元素","D":"通过 add 方法可以向列表添加元素"}, "a":"D", "desc":"Python列表没有 add 方法，集合(set)才有。"},
  {"id":55, "type":"单选题", "q":"下列选项中,关于设置代理服务器的描述错误的是（）。", "opts":{"A":"降低单个IP访问频率","B":"防止IP被封禁","C":"加快访问网站的速度","D":"代理IP的寿命是有限的"}, "a":"C", "desc":"使用代理通常会因为中转而略微降低访问速度，而不是加快。"},
  {"id":56, "type":"单选题", "q":"以下 Python语言关键字在异常处理结构中用来捕获特定类型异常的选项是（）。", "opts":{"A":"for","B":"lambda","C":"in","D":"except"}, "a":"D", "desc":""},
  {"id":57, "type":"单选题", "q":"在匹配嵌套了HTML内容的文本时,会忽略HTML内容本身存在的层次结构的解析语言是（）。", "opts":{"A":"正则表达式","B":"XPath","C":"Beautiful Soup","D":"所有选项均正确"}, "a":"A", "desc":""},
  {"id":58, "type":"单选题", "q":"print(json.dumps(demo_dict, ensure_ascii=False))，demo_dict = {\"city\": \"北京\", \"name\":\"小明\"}。运行程序,程序最终输出的结果为（）。", "opts":{"A":"{\"city\":\"北京\",\"name\":\"小明\"}","B":"{\"city\": \"\\u5317\\u4eac\", \"name\": \"\\u5c0f\\u660e\"}","C":"{}","D":"运行错误"}, "a":"A", "desc":"ensure_ascii=False 使得中文能正常显示而不是转义。"},
  {"id":59, "type":"单选题", "q":"https的端口号是（）。", "opts":{"A":"80","B":"8080","C":"443","D":"433"}, "a":"C", "desc":""},
  {"id":60, "type":"单选题", "q":"下列哪个正则表达式与 1\\d{5,9} 不相同（）。", "opts":{"A":"[1]\\d{5,9}","B":"1[0-9]{5,9}","C":"1[0123456789]{5,9}","D":"[1]\\D{5,9}"}, "a":"D", "desc":"\\D 匹配非数字，与 \\d 相反。"},
  {"id":61, "type":"单选题", "q":"下列不能匹配任意字符的正则表达式是（）。", "opts":{"A":"[\\d\\D]","B":"[\\w\\W]","C":"[\\s\\S]","D":"[\\a\\A]"}, "a":"D", "desc":"\\a 不是标准元字符，[\\a\\A] 不能匹配所有字符。"},
  {"id":62, "type":"单选题", "q":"下列选项中,表示向服务器提交表单或上传文件的请求方法是（）。", "opts":{"A":"GET","B":"POST","C":"HEAD","D":"PUT"}, "a":"B", "desc":""},
  {"id":63, "type":"单选题", "q":"下列选项中,用于以二进制形式获取响应内容的属性是（）。", "opts":{"A":"status_code","B":"text","C":"content","D":"string"}, "a":"C", "desc":""},
  {"id":64, "type":"单选题", "q":"bs4中,若已找到节点并存放于变量x中,能获取节点内容的是（）。", "opts":{"A":"x.text","B":"x.content","C":"x.html","D":"x.attrs"}, "a":"A", "desc":""},
  {"id":65, "type":"单选题", "q":"以下XPath谓语中,能获得满足条件的第一个节点的是（）。", "opts":{"A":"[0]","B":"[1]","C":"[first()]","D":"[min()+1]"}, "a":"B", "desc":"XPath索引从1开始。"},
  {"id":66, "type":"单选题", "q":"lxml库中,用于解析xml文件的方法是（）。", "opts":{"A":"etree.parse()","B":"etree.XML()","C":"etree.HTML()","D":"etree.fromstring()"}, "a":"A", "desc":"parse用于解析文件，其他通常用于解析字符串。"},
  {"id":67, "type":"单选题", "q":"关于JSONPath的描述,说法错误的是（）。", "opts":{"A":"JSONPath只适用于JSON文档","B":"JSONPath提供了描述JSON文档层次结构的表达式","C":"JSONPath提供的语法与XPath提供的语法相同","D":"JSONPath可以看作定位目标对象位置的语言"}, "a":"C", "desc":"JSONPath借鉴了XPath，但语法并不完全相同。"},
  {"id":68, "type":"单选题", "q":"requests库中,proxies参数传入一个字典,该字典中包含了所需要的代理IP,其中字典的键为（）。", "opts":{"A":"get","B":"ip地址","C":"协议类型(http或https)","D":"post"}, "a":"C", "desc":"键是协议名，如 'http' 或 'https'。"},
  {"id":69, "type":"单选题", "q":"URL地址'https://www.baidu.com?ie=utf-8&wd=python',其中属于表示查询字符串的是（）。", "opts":{"A":"ie=utf-8&wd=python","B":"https","C":"www.baidu.com","D":"wd=python"}, "a":"A", "desc":""},
  {"id":70, "type":"单选题", "q":"关于CSS选择器的描述,说法错误的是（）。", "opts":{"A":"类别选择器是根据类名选择元素,类名前面用“.”进行标注","B":"ID选择器是根据特定ID选择元素,ID前面加上“$”进行标注","C":"属性选择器是根据元素的属性选择元素,属性必须用中括号进行包裹","D":"元素选择器是根据元素名称选择元素"}, "a":"B", "desc":"ID选择器使用 # 标注，不是 $。"},
  {"id":71, "type":"单选题", "q":"以下选项中是HTTP请求行的是（）。", "opts":{"A":"GET / HTTP/1.1","B":"Connection: keep-alive","C":"Accept-Language: zh-CN","D":"User-Agent: Mozilla/5.0..."}, "a":"A", "desc":""},
  {"id":72, "type":"单选题", "q":"GET请求方法通过请求参数传输数据,最多能传输的数据量是（）。", "opts":{"A":"2KB","B":"4KB","C":"1M","D":"无限制"}, "a":"A", "desc":"通常浏览器限制在2KB左右。"},
  {"id":73, "type":"单选题", "q":"关于Beautiful Soup的描述,说法错误的是（）。", "opts":{"A":"Beautiful Soup是一个用于从HTML或XML文档中提取目标数据的Python库","B":"Beautiful Soup支持CSS选择器","C":"Beautiful Soup可以将HTML或XML文档、片段转换成节点树","D":"Beautiful Soup会将整个节点树看作一个Python类的对象"}, "a":"D", "desc":"它将节点树中的每个节点看作Python对象，而不是只把整个树看作一个对象。"},
  {"id":74, "type":"单选题", "q":"selenium中,浏览器对象往网页的输入框中输入文字需要调用的方法是（）。", "opts":{"A":"get()","B":"save_screenshot()","C":"send_keys()","D":"find_element_by_id()"}, "a":"C", "desc":""},
  {"id":75, "type":"单选题", "q":"selenium中,用于关闭浏览器对象的是（）。", "opts":{"A":"driver.cancel()","B":"driver.exit()","C":"driver.quit()","D":"driver.close()"}, "a":"C", "desc":"quit()关闭所有窗口并退出驱动，close()只关闭当前窗口。"},
  {"id":76, "type":"单选题", "q":"selenium中,能查找<form name='hello'></form>的元素的表达式是（）。", "opts":{"A":"find_element_by_css_selector('hello')","B":"find_element_by_class_name('hello')","C":"find_element_by_tag_name('hello')","D":"find_element_by_name('hello')"}, "a":"D", "desc":""},
  {"id":77, "type":"单选题", "q":"下列选项中,关于进程的描述错误的是（）。", "opts":{"A":"进程是系统进行资源分配的最小单位","B":"进程拥有自己的内存空间","C":"进程之间数据不共享","D":"进程的存在必须依赖线程"}, "a":"D", "desc":"进程不一定依赖线程，早期系统只有进程。"},
  {"id":78, "type":"单选题", "q":"下列选项中,关于多线程爬虫的述错误的是（）。", "opts":{"A":"开启的线程数量越多,程序运行速度越快","B":"多线程爬虫可以有效利用CPU和网络I/O等待时间","C":"多线程爬虫使用队列是为了保证安全地使用多线程采集网页数据","D":"通常情况下,多线程爬虫会开启多个线程抓取网页和解析网页"}, "a":"A", "desc":"线程过多会导致切换开销大，性能反而下降。"},
  {"id":79, "type":"单选题", "q":"以下哪个是多线程爬虫中,用于等待子线程结束的方法?（）。", "opts":{"A":"start()","B":"join()","C":"setDaemon()","D":"run()"}, "a":"B", "desc":""},
  {"id":80, "type":"单选题", "q":"在Python中,以下哪个选项用于创建一个线程对象?（）。", "opts":{"A":"threading.create_thread()","B":"threading.Thread()","C":"threading.new_thread()","D":"threading.start_thread()"}, "a":"B", "desc":""},
  {"id":81, "type":"单选题", "q":"在Python中,通过继承Thread类创建自定义线程时,需要重写哪个方法?（）。", "opts":{"A":"_init_()","B":"start()","C":"run()","D":"execute()"}, "a":"C", "desc":""},
  {"id":82, "type":"单选题", "q":"下列选项中,关于Scrapy框架的描述正确的是（）。", "opts":{"A":"Scrapy是一个纯使用Python语言开发的收费的网络爬虫框架","B":"Scrapy支持Shell工具,方便开发人员独立调试程序","C":"Scrapy自身可以实现分布式爬虫","D":"Scrapy是基于Scrapy-Splash框架开发的"}, "a":"B", "desc":"A错误(开源免费)，C错误(需配合其他组件)，D错误(反了)。"},
  {"id":83, "type":"判断题", "q":"爬虫的合法性完全取决于网站的robots.txt文件。", "a":"错", "desc":"还涉及法律法规。"},
  {"id":84, "type":"判断题", "q":"互联网上每个文件都有一个唯一的URL。", "a":"对", "desc":""},
  {"id":85, "type":"判断题", "q":"一个IP地址只能对应一个域名。", "a":"错", "desc":"一个IP可以对应多个域名（虚拟主机）。"},
  {"id":86, "type":"判断题", "q":"HTTP协议是无状态的,这意味着每次请求都是独立的,不会记住之前的请求信息。", "a":"对", "desc":""},
  {"id":87, "type":"判断题", "q":"爬虫在抓取动态内容时,如果直接使用 requests 库获取网页源码,能够获得所有渲染后的内容。", "a":"错", "desc":"Requests获取的是服务器原始响应，不包含JS渲染内容。"},
  {"id":88, "type":"判断题", "q":"get方法比post方法速度更快。", "a":"对", "desc":"通常情况。"},
  {"id":89, "type":"判断题", "q":"一次HTTP通信的过程包括HTTP请求和HTTP响应。", "a":"对", "desc":""},
  {"id":90, "type":"判断题", "q":"在HTTP请求中,GET方法比POST方法更适合传输大量数据。", "a":"错", "desc":"POST更适合大量数据。"},
  {"id":91, "type":"判断题", "q":"在HTTP请求头中,Referer 字段会告知服务器请求的来源页面,能帮助网站分析流量来源。", "a":"对", "desc":""},
  {"id":92, "type":"判断题", "q":"当服务器返回HTTP状态码404时,表示请求的资源没有找到。", "a":"对", "desc":""},
  {"id":93, "type":"判断题", "q":"爬虫可以通过模拟点击事件来动态获取数据,但如果没有正确处理JavaScript渲染,它仍然无法抓取数据。", "a":"对", "desc":""},
  {"id":94, "type":"判断题", "q":"Python中的open()函数可以用于打开网页并读取网页内容。", "a":"错", "desc":"open()用于本地文件，网页需要requests/urllib。"},
  {"id":95, "type":"判断题", "q":"requests.get()方法可以用来发送GET请求并返回一个包含响应内容的对象。", "a":"对", "desc":""},
  {"id":96, "type":"判断题", "q":"在XPath中,//input[@type='text']可以选择所有 type='text'的input元素,无论其层级如何。", "a":"对", "desc":""},
  {"id":97, "type":"判断题", "q":"在XPath中,//a[starts-with(@href, 'https')]会选择所有href属性以https开头的a标签。", "a":"对", "desc":""},
  {"id":98, "type":"判断题", "q":"在XPath中,//div[@id='content']/text()可以选取id='content'的div标签中的所有文本节点。", "a":"错", "desc":"只能选取直接子文本节点，不包括后代。"},
  {"id":99, "type":"判断题", "q":"XPath表达式//div[@class='header'] [contains (text(), 'Python')]会选择class='header'且包含文本 Python的div标签。", "a":"对", "desc":""},
  {"id":100, "type":"判断题", "q":"在XPath中,//div[@class='header']/*[2]会选择class='header'的div标签下的第二个子元素,不管它是什么类型的标签。", "a":"对", "desc":""},
  {"id":101, "type":"判断题", "q":"soup.select('div p#main')会选择所有 div标签中的 id='main'的p标签。", "a":"对", "desc":""},
  {"id":102, "type":"判断题", "q":"soup.select('div.header > p')会选择所有类名为 header 的div元素的直接子元素p。", "a":"错", "desc":"题目描述可能有误，原文说如果有空格表示后代，这里语法是 > 表示直接子元素，是正确的。但根据文档[464]的解释，它似乎在强调空格的问题，结合文档上下文，该题判断为错。"},
  {"id":103, "type":"判断题", "q":"soup.select('div#header')用于选取页面中第一个id='header'的div标签。", "a":"错", "desc":"select返回列表，包含所有符合条件的。"},
  {"id":104, "type":"判断题", "q":"在BeautifulSoup中, soup.find_all('div', {'class': 'header'})与soup.find_all('div', class_='header')效果是相同的。", "a":"对", "desc":""},
  {"id":105, "type":"判断题", "q":"在BeautifulSoup中,.get_text()方法会返回标签内所有文本内容,但不包括任何子标签的内容。", "a":"错", "desc":"包括子标签内容。"},
  {"id":106, "type":"判断题", "q":"soup.find('p', {'class': 'intro'}).find('a')会选取class='intro'的p标签下第一个标签。", "a":"对", "desc":""},
  {"id":107, "type":"判断题", "q":"soup.find('div', class_='header').find_all('p')可以选取class='header'的div标签下所有p标签。", "a":"对", "desc":""},
  {"id":108, "type":"判断题", "q":"在BeautifulSoup中, soup.find_all('div', class_='header') [1]会返回所有class='header'的div标签中的第一个元素。", "a":"错", "desc":"[1]返回第二个元素。"},
  {"id":109, "type":"判断题", "q":"在BeautifulSoup中, soup.select('div#content p')与soup.find_all('p', {'class': 'content'})效果相同。", "a":"错", "desc":"前者是id为content的div下的p，后者是class为content的p，逻辑不同。"},
  {"id":110, "type":"判断题", "q":"Selenium不支持浏览器的功能,它不需要与第三方浏览器结合使用。", "a":"错", "desc":"需要驱动浏览器。"},
  {"id":111, "type":"判断题", "q":"表层网页是指传统搜索引擎可以索引的页面,主要以超链接可以到达的静态网页构成的网页。", "a":"对", "desc":""},
  {"id":112, "type":"判断题", "q":"POST请求的请求参数会暴露在URL地址中。", "a":"错", "desc":"GET才会。"},
  {"id":113, "type":"判断题", "q":"JSON比XML的语法更简单,层次结构更加清晰,易于阅读。", "a":"对", "desc":""},
  {"id":114, "type":"判断题", "q":"使用Selenium可以抓取动态网页中的数据。", "a":"对", "desc":""},
  {"id":115, "type":"判断题", "q":"Requests是基于urllib3编写的库。", "a":"对", "desc":""},
  {"id":116, "type":"判断题", "q":"服务器端可以记住用户的登录状态,因此HTTP协议自身具有保持会话状态的功能。", "a":"错", "desc":"HTTP协议本身无状态。"},
  {"id":117, "type":"判断题", "q":"Selenium启动浏览器后,浏览器的窗口默认以最大化的形式显示。", "a":"错", "desc":"默认不是最大化。"},
  {"id":118, "type":"判断题", "q":"JSONPath只能解析JSON格式的数据。", "a":"对", "desc":""},
  {"id":119, "type":"判断题", "q":"re模块在提取HTML标签中的特定信息时比BeautifulSoup 或lxml会更高效。", "a":"错", "desc":"正则处理复杂HTML结构通常不如专用解析库高效稳定。"},
  {"id":120, "type":"判断题", "q":"POST请求方法的参数信息会在URL地址中显示。", "a":"错", "desc":"同112题。"},
  {"id":121, "type":"判断题", "q":"在XPath中,@*表示选取所有属性,而//*表示选取所有节点。", "a":"对", "desc":""},
  {"id":122, "type":"判断题", "q":"HTTPS协议在传输数据过程中比HTTP协议更加安全。", "a":"对", "desc":""},
  {"id":123, "type":"判断题", "q":"JSONPath表达式$..*可以用来获取JSON对象中所有字段的值。", "a":"对", "desc":""},
  {"id":124, "type":"判断题", "q":"Python中可以使用open()函数将图片数据写入磁盘,其操作模式为'w'。", "a":"错", "desc":"需用 'wb'。"},
  {"id":125, "type":"判断题", "q":"Robots协议可以从根本上约束爬虫程序。", "a":"错", "desc":"没有强制约束力。"},
  {"id":126, "type":"判断题", "q":"get方法相比post方法能携带更多信息。", "a":"错", "desc":"POST能携带更多。"},
  {"id":127, "type":"判断题", "q":"XPath中,路径表达式是唯一的。", "a":"错", "desc":"同一节点可以有多种表达方式。"},
  {"id":128, "type":"判断题", "q":"CSS用于向网页中添加交互行为。", "a":"错", "desc":"JS用于交互，CSS用于样式。"},
  {"id":129, "type":"判断题", "q":"如果服务器返回的状态码为500,则表示客户端发送的请求出现错误。", "a":"错", "desc":"500是服务器错误，4xx是客户端错误。"},
  {"id":130, "type":"判断题", "q":"在使用Selenium时,.get()方法可以加载页面并等待页面完全加载后返回控制权,但不能用于抓取JavaScript渲染的数据。", "a":"错", "desc":"Selenium主要优势就是抓取JS渲染数据。"},
  {"id":131, "type":"判断题", "q":"在实际应用中,文件存储和数据库存储各有利弊,文件存储比较适合中小型网络爬虫,数据库存储比较适合大型网络爬虫。", "a":"对", "desc":""},
  {"id":132, "type":"判断题", "q":"线程具有独立运行、状态不可测、执行顺序随机的特点。", "a":"对", "desc":""},
  {"id":133, "type":"判断题", "q":"线程是系统进行资源分配的最小单位。", "a":"错", "desc":"进程是资源分配最小单位，线程是调度最小单位。"},
  {"id":134, "type":"判断题", "q":"线程共享同一进程中的数据。", "a":"对", "desc":""},
  {"id":135, "type":"判断题", "q":"一个Scrapy项目可以包含多个爬虫。", "a":"对", "desc":""},
  {"id":136, "type":"简答题", "q":"请列举并阐述网络爬虫的常见分类。", "a":"通用网络爬虫:又称全网爬虫,是指访问全互联网资源的网络爬虫。\n聚焦网络爬虫:又称主题网络爬虫,是指有选择性地访问那些与预定主题相关网页的网络爬虫,它根据预先定义好的目标,获取所需要的数据。\n增量式网络爬虫:是指对已下载的网页采取增量式更新,只抓取新产生或者已经发生变化的网页的网络爬虫。\n深层网络爬虫:是指抓取深层网页的网络爬虫,它要抓取的网页层次比较深,需要通过一定的附加策略才能够自动抓取,实现难度较大。", "desc":""},
  {"id":137, "type":"简答题", "q":"XPATH路径表达式由哪些符号组成,请列举并描述它们的含义。", "a":"/:表示从当前节点选择直接子节点。\n//:表示从当前节点选择子孙节点,忽略层级关系。\n.:表示选取当前节点。\n..:表示选取当前节点的父节点。\n@:选取属性节点。\n[]:表示谓词。", "desc":""},
  {"id":138, "type":"简答题", "q":"请简述浏览器访问百度首页的加载过程。", "a":"1. 当我们在地址栏输入百度官网地址后,浏览器首先通过DNS服务器查找百度服务器对应的IP地址;\n2. 接着浏览器向IP地址对应的Web服务器发送HTTP请求;\n3. 然后Web服务器接收HTTP请求后进行处理,向浏览器返回HTML页面;\n4. 最后浏览器对HTML页面进行渲染呈现给用户。", "desc":""},
  {"id":139, "type":"简答题", "q":"请简要说明什么是Cookie,它有什么用途?", "a":"Cookie 是一种在用户计算机上存储小型文本文件的技术,用于在用户与网站进行交互时收集和存储有关用户的信息。\n当用户访问一个网站时,网站会将一个包含特定信息的Cookie 文件发送到用户的浏览器,浏览器会将该 Cookie 存储在用户的计算机上。之后,当用户再次访问该网站时,浏览器会向服务器发送 Cookie,服务器可以根据Cookie 中的信息来识别用户、跟踪用户行为等。", "desc":""},
  {"id":140, "type":"简答题", "q":"请求方法GET和POST有哪些主要区别。", "a":"GET用于请求服务器发送某个资源,POST用于向服务器提交表单或上传文件。\n(数据大小方面)GET请求方法通过请求参数传输数据,最多只能传输2KB的数据;POST请求方法通过实体内容传输数据,可以传输的数据大小没有限制。\n(安全性方面)GET请求方法的参数信息会在URL中明文显示,安全性比较低;POST请求方法传递的参数会隐藏在实体内容中,用户看不到,安全性更高。", "desc":""},
  {"id":141, "type":"简答题", "q":"常见的反爬虫措施有哪些,请列举并简述应对方法。", "a":"1. 用户身份检查:在请求网页时携带User-Agent,将自己伪装成一个浏览器。\n2. IP黑名单(封IP):降低网络爬虫访问网站的频率;或者使用代理服务器。\n3. 验证码:使用图像识别等技术识别验证码。\n4. 其他:Cookie验证、网页动态加载、字体反爬、js反爬、数据加密等。", "desc":""},
  {"id":142, "type":"简答题", "q":"请简述使用requests库进行网页数据抓取的一般流程。", "a":"1. 观察网页结构,分析网站与浏览器之间的网络交互,确定包含要抓取的数据的URL。\n2. 确定请求方式,获取请求头信息,分析请求携带的参数信息。使用requests库模拟浏览器向服务器发送请求,获取静态页面数据。\n3. 对获取到的网页源码进行解析,以获取目标数据。\n4. 将得到的数据进行处理,并存放在本地文件或数据库中。", "desc":""},
  {"id":143, "type":"简答题", "q":"请列举4种用selenium定位单个元素的方法并进行简单说明。", "a":"通过ID定位: driver.find_element(By.ID, 'element_id')\n通过xpath定位: driver.find_element(By.XPATH, '//div[@class=\"simple\"]')\n通过CSS选择器定位: driver.find_element(By.CSS_SELECTOR, '#element_id')\n通过标签名定位: driver.find_element(By.TAG_NAME, 'div')\n通过name属性: driver.find_element(By.NAME, 'element_name')\n通过链接文本: driver.find_element(By.LINK_TEXT, 'Link Text')", "desc":""},
  {"id":144, "type":"简答题", "q":"请简述robots的作用,并列举和阐述robots.txt文件中的选项。", "a":"Robots协议用于保护网站数据和敏感信息,确保网站用户的个人信息和隐私不受侵犯。\nUser-agent:适用对象(用户代理),若该选项的值为“*”,则说明对任何网络爬虫均有效。\nDisallow:不允许访问的目录或文件\nAllow:允许访问的目录或文件\nSitemap:站点地图,告知网络爬虫网站地图的路径。", "desc":""},
  {"id":145, "type":"简答题", "q":"常见的CSS选择器有哪些,请举例说明。", "a":"类别选择器: .intro\nID选择器: #link1\n属性选择器: [target=blank]\n后代选择器: div p (选择div下的所有p)\n子代选择器: div > p (选择div下的直接子元素p)", "desc":""},
  {"id":146, "type":"简答题", "q":"请简述多线程爬虫的运行流程。", "a":"1. 构建一个网址队列,用于存放网络爬虫待抓取数据的所有网址。\n2. 开启多个线程抓取网页。\n3. 将抓取到的网页源码存储到网页源码队列中。\n4. 开启多个线程对网页源码队列中的网页数据进行解析。\n5. 将解析之后的数据存储到网页数据队列中。\n6. 将网页数据队列中的数据进行存储。", "desc":""},
  {"id":147, "type":"简答题", "q":"请简述文件存储和数据库存储的特点。", "a":"文件存储: 基础的数据存储方式,将数据以文件的形式存储到本地电脑中,适合中小型网络爬虫。\n数据库: 可以对数据进行分类保存,有效地避免存储重复的数据,提供了高效的检索和访问方式,适合大型网络爬虫。", "desc":""},
  {"id":148, "type":"简答题", "q":"请简述Scrapy框架的优点和缺点。", "a":"优点: (1)文档丰富,社区良好。(2)支持并发,速度快。(3)XPath解析快。(4)统一中间件。(5)支持Shell调试。(6)管道存储灵活。(7)高度可定制。\n缺点: (1)自身无法实现分布式爬虫。(2)去重效果差,不能持久化。(3)无法获取JS动态渲染的页面内容。", "desc":""},
  {"id":149, "type":"操作题", "q":"编写一个Python程序,使用requests模块发送一个GET请求到http://httpbin.org/get,并打印出相应的text内容。", "a":"import requests\nresponse = requests.get('http://httpbin.org/get')\nprint(response.text)", "desc":""},
  {"id":150, "type":"操作题", "q":"编写一个Python程序,将一个包含数据的字典写入output.json文件中,并确保输出的JSON字符串是格式化的(缩进为4个单位)。\ndata = {\"title\": \"Sample Data\", \"author\": \"Charlie\", \"publication_year\": 2022, \"genres\": [\"Fiction\", \"Adventure\"]}", "a":"import json\n\ndata = {\n    \"title\": \"Sample Data\",\n    \"author\": \"Charlie\",\n    \"publication_year\": 2022,\n    \"genres\": [\"Fiction\", \"Adventure\"]\n}\n\nwith open('output.json', 'w') as file:\n    json.dump(data, file, indent=4, sort_keys=True)", "desc":""},
  {"id":151, "type":"操作题", "q":"现有html代码字符串(html变量),请用XPath提取第一个商品名称、第二个商品价格、第二个商品链接,分别保存到product1_name, product2_price, product2_link。\n\nhtml ='''<html>...</html>''' (见题干)", "a":"from lxml import etree\ntree = etree.HTML(html)\nproduct1_name = tree.xpath('//div[@class=\"product\"][1]//h2[@class=\"name\"]/text()')[0]\nproduct2_price = tree.xpath('//div[@class=\"product\"][2]//p[@class=\"price\"]/text()')[0]\nproduct2_link = tree.xpath('//div[@class=\"product\"][2]//a/@href')[0]", "desc":""},
  {"id":152, "type":"操作题", "q":"接上题,请使用BeautifulSoup实现同样的功能。", "a":"from bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nproduct1_name = soup.select('.product > .name')[0].text\nproduct2_price = soup.select('.product > .price')[1].text\nproduct2_link = soup.select('.product > a')[1].get('href')", "desc":""},
  {"id":153, "type":"操作题", "q":"现有html表格代码,请使用bs4获取排名、学校中文名称、省份、类型信息,存入rank, name, address, category。", "a":"from bs4 import BeautifulSoup\nsoup = BeautifulSoup(html, 'lxml')\nrank = soup.select('.rank')[0].text\nname = soup.select('.name.cn')[0].text\naddress = soup.select('#address')[0].text\ncategory = soup.select('span')[0].text\nprint(rank, name, address, category)", "desc":""},
  {"id":154, "type":"操作题", "q":"接上题,请使用lxml(XPath)实现同样的功能。", "a":"from lxml import etree\ntree = etree.HTML(html)\nrank = tree.xpath('//td[@class=\"rank\"]/text()')[0]\nname = tree.xpath('//a[@class=\"name cn\"]/text()')[0]\naddress = tree.xpath('//td[@id=\"address\"]/text()')[0]\ncategory = tree.xpath('//span/text()')[0]\nprint(rank, name, address, category)", "desc":""},
  {"id":155, "type":"操作题", "q":"现有HTML页面源代码html_content,请使用xpath: 1)创建节点树 2)提取标题 3)提取描述 4)提取所有链接href。", "a":"from lxml import etree\ntree = etree.HTML(html_content)\n# 2\ntitle = tree.xpath('//title/text()')[0]\nprint(f\"页面标题:{title}\")\n# 3\ndescription = tree.xpath('//p[@class=\"description\"]/text()')[0]\nprint(f\"页面描述:{description}\")\n# 4\nlinks = tree.xpath('//a/@href')\nfor link in links:\n    print(f\"链接:{link}\")", "desc":""},
  {"id":156, "type":"操作题", "q":"编写Selenium脚本:打开http://www.example.com,定位id='input_id'输入框并输入'Hello, Selenium!',然后点击id='button_id'的按钮。", "a":"from selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\ndriver = webdriver.Chrome()\ntry:\n    driver.get(\"http://www.example.com\")\n    input_element = driver.find_element(By.ID, \"input_id\")\n    input_element.send_keys(\"Hello, Selenium!\")\n    button_element = driver.find_element(By.ID, \"button_id\")\n    button_element.click()\nfinally:\n    driver.quit()", "desc":""},
  {"id":157, "type":"操作题", "q":"目标网站共50页,爬取html并提取楼盘名称、地址、面积、房价。请补全参考代码中的download函数部分。", "a":"def download(text):\n    tree = etree.HTML(text)\n    name_list = tree.xpath('//li[@class=\"list\"]/a/img/@alt')\n    address_list = tree.xpath('//div[@class=\"address\"]/text()')\n    size_list = tree.xpath('//div[@class=\"size\"]/text()')\n    price_list = tree.xpath('//div[@class=\"msg_box\"]/div/span/text()')\n    result_list = []\n    for i in range(len(name_list)):\n        name = name_list[i]\n        address = address_list[i]\n        size = size_list[i]\n        price = price_list[i]\n        result_list.append([name, address, size, price])", "desc":""},
  {"id":158, "type":"操作题", "q":"编写爬虫,爬取新浪新闻头条的标题、时间和链接,写入excel。请补全parse_html函数的逻辑。", "a":"def parse_html(text):\n    item = []\n    tree = etree.HTML(text)\n    title_list = tree.xpath('//h2[@class=\"news-title\"]/text()')\n    time_list = tree.xpath('//p[@class=\"news-time\"]/text()')\n    link_list = tree.xpath('//a/@href')\n    for i in range(len(title_list)):\n        title = title_list[i]\n        time = time_list[i]\n        link = link_list[i]\n        item.append({\n            '新闻标题': title,\n            '发布时间': time,\n            '新闻链接': link\n        })\n    return item", "desc":""},
  {"id":159, "type":"操作题", "q":"爬取站长素材网风景图片标题和链接,保存到Excel。请补全parse_html函数。", "a":"def parse_html(text, item):\n    tree = etree.HTML(text)\n    title_list = tree.xpath('//div[@class=\"item\"]/img/@alt')\n    src_list = tree.xpath('//div[@class=\"item\"]/img/@src')\n    for i in range(len(title_list)):\n        title = title_list[i]\n        src = src_list[i]\n        item.append({\n            'Image Title': title,\n            'Image URL': src\n        })", "desc":""},
  {"id":160, "type":"简答题", "q":"请论述在网络爬虫开发中需要考虑哪些法律与伦理问题?作为一名开发者,应如何遵守相关规范?", "a":"法律上: 不得侵犯著作权、个人信息及商业秘密,严禁非法侵入系统或扰乱网站运营。\n伦理上: 尊重网站意愿(Robots协议)、节约网络资源(控制频率),保证数据使用透明公正。\n作为开发者: 事前合规审查,技术上友好爬取、最小化收集数据并妥善保护,事后持续监控行为。", "desc":""},
  {"id":161, "type":"简答题", "q":"请简述你写网络爬虫程序的基本思路或流程,并举例说明爬虫技术在实际应用中的几种常见场景。", "a":"流程: 抓包分析 -> 获取静态页面(Requests) -> 应对反爬(User-Agent/IP代理) -> HTML解析(XPath/BS4) -> 数据存储。\n应用场景: 搜索引擎索引、电商价格监控、舆情分析。", "desc":""},
  {"id":162, "type":"简答题", "q":"这学期学了哪些解析网页数据的技术?请列举并对不同技术的应用场景或者优缺点进行简单分析说明。", "a":"XPath: 语法通用,解析速度快,适合结构化文档。\nBeautifulSoup: 容错率高,API简单易用,但速度稍慢,适合处理不规范HTML。\nJSONPath: 专门解析JSON数据,语法简洁。\n正则表达式: 灵活强大,适合提取特定模式的字符串,但编写复杂,容易出错。", "desc":""},
  {"id":163, "type":"简答题", "q":"请探讨requests和selenium的技术特性差异,并各举一个典型应用场景。", "a":"Requests: 直接发送HTTP请求,速度快,资源消耗少,但无法处理JS渲染的动态内容。适合抓取静态网页或API接口。\nSelenium: 模拟真实浏览器操作,能处理JS渲染和复杂交互,但速度慢,资源消耗大。适合抓取动态加载、需要登录验证的复杂网页。", "desc":""}
]
